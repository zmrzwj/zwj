em算法指的是最大期望算法（Expectation Maximization Algorithm，又译期望最大化算法），是一种迭代算法，用于含有隐变量（latent variable）的概率参数模型的最大似然估计或极大后验概率估计。
最大期望算法经过两个步骤交替进行计算：
第一步是计算期望（E），利用概率模型参数的现有估计值，计算隐藏变量的期望；
第二步是最大化（M），利用E 步上求得的隐藏变量的期望，对参数模型进行最大似然估计。
M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。
总体来说，EM的算法流程如下：
1.初始化分布参数
2.重复直到收敛：
E步骤：估计未知参数的期望值，给出当前的参数估计。
M步骤：重新估计分布参数，以使得数据的似然性最大，给出未知变量的期望估计。

EM算法简述
迭代使用EM步骤，直至收敛。
可以有一些比较形象的比喻说法把这个算法讲清楚。比如说食堂的大师傅炒了一份菜，要等分成两份给两个人吃，显然没有必要拿来天平一点一点的精确的去称分量，最简单的办法是先随意的把菜分到两个碗中，然后观察是否一样多，把比较多的那一份取出一点放到另一个碗中，这个过程一直迭代地执行下去，直到大家看不出两个碗所容纳的菜有什么分量上的不同为止。EM算法就是这样，假设我们估计知道A和B两个参数，在开始状态下二者都是未知的，并且知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。
EM 算法是 Dempster，Laind，Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据(incomplete data)。

EM算法的主要目的是提供一个简单的迭代算法计算后验密度函数，它的最大优点是简单和稳定，但容易陷入局部最优。
